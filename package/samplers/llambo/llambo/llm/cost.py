import os
from typing import Dict
from typing import List
from typing import Optional

import tiktoken
import transformers

from .prompter import PromptBase


class Calculator:
    # Pricing per 1M input tokens in USD for GPT models
    GPT_input_pricing: Dict[str, float] = {
        "gpt-4o": 2.50,
        "gpt-4o-2024-11-20": 2.50,
        "gpt-4o-2024-08-06": 2.50,
        "gpt-4o-2024-05-13": 5.00,
        "gpt-4o-mini": 0.15,
        "gpt-4o-mini-2024-07-18": 0.15,
        "o1-preview": 15.00,
        "o1-preview-2024-09-12": 15.00,
        "o1-mini": 3.00,
        "o1-mini-2024-09-12": 3.00,
        "chatgpt-4o-latest": 5.00,
        "gpt-4-turbo": 10.00,
        "gpt-4-turbo-2024-04-09": 10.00,
        "gpt-4": 30.00,
        "gpt-4-32k": 60.00,
        "gpt-4-0125-preview": 10.00,
        "gpt-4-1106-preview": 10.00,
        "gpt-4-vision-preview": 10.00,
        "gpt-3.5-turbo-0125": 0.50,
        "gpt-3.5-turbo-instruct": 1.50,
        "gpt-3.5-turbo-1106": 1.00,
        "gpt-3.5-turbo-0613": 1.50,
        "gpt-3.5-turbo-16k-0613": 3.00,
        "gpt-3.5-turbo-0301": 1.50,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    DeepSeek_input_pricing: Dict[str, float] = {
        "deepseek-chat": 0.14,
        "deepseek-reasoner": 0.55,
    }  # assume always cache miss
    DeepSeek_output_pricing: Dict[str, float] = {
        "deepseek-chat": 0.28,
        "deepseek-reasoner": 2.19,
    }

    # Pricing per 1M output tokens in USD for GPT models
    GPT_output_pricing: Dict[str, float] = {
        "gpt-4o": 10.00,
        "gpt-4o-2024-11-20": 10.00,
        "gpt-4o-2024-08-06": 10.00,
        "gpt-4o-2024-05-13": 15.00,
        "gpt-4o-mini": 0.60,
        "gpt-4o-mini-2024-07-18": 0.60,
        "o1-preview": 60.00,
        "o1-preview-2024-09-12": 60.00,
        "o1-mini": 12.00,
        "o1-mini-2024-09-12": 12.00,
        "chatgpt-4o-latest": 15.00,
        "gpt-4-turbo": 30.00,
        "gpt-4-turbo-2024-04-09": 30.00,
        "gpt-4": 60.00,
        "gpt-4-32k": 120.00,
        "gpt-4-0125-preview": 30.00,
        "gpt-4-1106-preview": 30.00,
        "gpt-4-vision-preview": 30.00,
        "gpt-3.5-turbo-0125": 1.50,
        "gpt-3.5-turbo-instruct": 2.00,
        "gpt-3.5-turbo-1106": 2.00,
        "gpt-3.5-turbo-0613": 2.00,
        "gpt-3.5-turbo-16k-0613": 4.00,
        "gpt-3.5-turbo-0301": 2.00,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    def __init__(
        self,
        model: str,
        formatted_input_sequence: Optional[List[Dict[str, str]]] = None,
        output_sequence_string: Optional[str] = None,
    ) -> None:
        self.model = model
        self.formatted_input_sequence = formatted_input_sequence
        self.output_sequence_string = output_sequence_string
        self.input_token_length: int = 0
        self.output_token_length: int = 0

    def calculate_input_token_length_GPT(self) -> int:
        """Calculate the number of tokens used by a list of messages."""
        try:
            encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print(
                "Warning: model not found for tokenization. Using cl100k_base encoding for cost "
                "calculation."
            )
            encoding = tiktoken.get_encoding("cl100k_base")

        tokens_per_message = 3
        tokens_per_name = 1

        num_tokens = 0
        if self.formatted_input_sequence:
            for message in self.formatted_input_sequence:
                num_tokens += tokens_per_message
                for key, value in message.items():
                    num_tokens += len(encoding.encode(value))
                    if key == "name":
                        num_tokens += tokens_per_name
        num_tokens += 3  # Every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    def calculate_output_token_length_GPT(self) -> int:
        """
        Calculates the number of tokens in a given output sequence.

        Parameters:
            output_sequence (str): The text output generated by the model.
            model (str): The name of the model for tokenization. Default is "gpt-3.5-turbo".

        Returns:
            int: The number of tokens in the output sequence.
        """
        try:
            # Initialize tokenizer for the specific model
            tokenizer = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print("Warning: Model not found. Using cl100k_base encoding as a fallback.")
            tokenizer = tiktoken.get_encoding("cl100k_base")

        # Tokenize the output sequence
        if self.output_sequence_string:
            tokens = tokenizer.encode(self.output_sequence_string)
            return len(tokens)
        return 0

    def calculate_cost_GPT(self) -> float:
        if self.formatted_input_sequence is not None:
            self.input_token_length = self.calculate_input_token_length_GPT()
            input_cost = self.input_token_length * self.GPT_input_pricing[self.model] / 1e6
        else:
            input_cost = 0
        if self.output_sequence_string is not None:
            self.output_token_length = self.calculate_output_token_length_GPT()
            output_cost = self.output_token_length * self.GPT_output_pricing[self.model] / 1e6
        else:
            output_cost = 0
        return input_cost + output_cost

    def calculate_token_length_DeepSeek(self) -> None:
        module_dir = os.path.dirname(os.path.abspath(__file__))

        tokenizer_relative_path = "tokenizers/deepseek"

        tokenizer_absolute_path = os.path.join(module_dir, tokenizer_relative_path)

        tokenizer = transformers.AutoTokenizer.from_pretrained(
            tokenizer_absolute_path, trust_remote_code=True
        )

        if self.formatted_input_sequence is not None:
            input_sequence = PromptBase.formatted_to_string_OpenAI(self.formatted_input_sequence)
            input_tokenized = tokenizer.encode(input_sequence)
            self.input_token_length = len(input_tokenized)

        if self.output_sequence_string is not None:
            output_tokenized = tokenizer.encode(self.output_sequence_string)
            self.output_token_length = len(output_tokenized)

    def calculate_cost_DeepSeek(self) -> float:
        self.calculate_token_length_DeepSeek()
        cost = (
            self.input_token_length * self.DeepSeek_input_pricing[self.model]
            + self.output_token_length * self.DeepSeek_output_pricing[self.model]
        )
        cost /= 1e6
        return cost

    def calculate_input_token_length(self, input_sequence: List[str], form: str = "list") -> int:
        if form == "list":
            self.formatted_input_sequence = PromptBase.list_to_formatted_OpenAI(input_sequence)
        elif form == "formatted":
            self.formatted_input_sequence = input_sequence  # type: ignore
        else:
            raise ValueError("Invalid form. Use 'list' or 'formatted'.")

        if self.model in self.GPT_input_pricing:
            self.input_token_length = self.calculate_input_token_length_GPT()
        elif self.model in self.DeepSeek_input_pricing:
            self.calculate_token_length_DeepSeek()
        else:
            raise ValueError(f"Model {self.model} not supported for token length calculation.")

        return self.input_token_length

    def length_limiter(
        self,
        input_sequence: List[str],
        limit: int,
        truncation: bool = True,
        include_truncation_warning: bool = True,
    ) -> List[str]:
        self.calculate_input_token_length(input_sequence, form="list")

        if self.input_token_length > limit:
            print(f"Warning: Input sequence is longer than {limit} tokens.")

            if truncation:
                # Calculate the token length of the warning message if it will be included
                warning_message = "---Warning, this information is too long and is truncated---"
                warning_token_length = (
                    self.calculate_input_token_length([warning_message], form="list")
                    if include_truncation_warning
                    else 0
                )

                # Adjust the limit to account for the warning message
                adjusted_limit = (
                    limit - warning_token_length if include_truncation_warning else limit
                )

                # Convert the input sequence to a single string for truncation
                input_string = " ".join(input_sequence)

                # Estimate the required reduction percentage
                required_reduction_ratio = adjusted_limit / self.input_token_length

                # Apply the estimated reduction
                truncated_length = int(len(input_string) * required_reduction_ratio)
                truncated_string = input_string[:truncated_length]

                # Split the truncated string back into a list of strings, preserving the original structure
                truncated_input_sequence: List[str] = []
                current_length = 0
                for sentence in input_sequence:
                    if current_length + len(sentence) + 1 <= truncated_length:
                        truncated_input_sequence.append(sentence)
                        current_length += len(sentence) + 1  # +1 for the space
                    else:
                        # Truncate the last sentence to fit within the limit
                        remaining_length = truncated_length - current_length
                        if remaining_length > 0:
                            truncated_sentence = sentence[:remaining_length]
                            truncated_input_sequence.append(truncated_sentence)
                        break

                # Re-calculate the token length to ensure it's within the adjusted limit
                self.calculate_input_token_length(truncated_input_sequence, form="list")

                # If the token count is still over the adjusted limit, adjust further iteratively
                while self.input_token_length > adjusted_limit:
                    # Re-estimate the required reduction ratio based on the current token count
                    required_reduction_ratio = adjusted_limit / self.input_token_length
                    truncated_length = int(len(truncated_string) * required_reduction_ratio)
                    truncated_string = truncated_string[:truncated_length]

                    # Rebuild the truncated input sequence
                    truncated_input_sequence = []
                    current_length = 0
                    for sentence in input_sequence:
                        if current_length + len(sentence) + 1 <= truncated_length:
                            truncated_input_sequence.append(sentence)
                            current_length += len(sentence) + 1
                        else:
                            remaining_length = truncated_length - current_length
                            if remaining_length > 0:
                                truncated_sentence = sentence[:remaining_length]
                                truncated_input_sequence.append(truncated_sentence)
                            break

                    # Re-calculate the token length
                    self.calculate_input_token_length(truncated_input_sequence, form="list")

                # Append the truncation warning if required
                if include_truncation_warning:
                    truncated_input_sequence.append(warning_message)

                # Re-calculate the final token length including the warning
                self.calculate_input_token_length(truncated_input_sequence, form="list")

                print(f"Warning: Input sequence is truncated to be about {limit} tokens.")
                return truncated_input_sequence
            else:
                return input_sequence
        else:
            return input_sequence
