import os
from typing import Dict
from typing import List
from typing import Optional

import tiktoken
import transformers

from .prompter import PromptBase


class Calculator:
    # Pricing per 1M input tokens in USD for GPT models
    GPT_input_pricing: Dict[str, float] = {
        "gpt-4o": 2.50,
        "gpt-4o-2024-11-20": 2.50,
        "gpt-4o-2024-08-06": 2.50,
        "gpt-4o-2024-05-13": 5.00,
        "gpt-4o-mini": 0.15,
        "gpt-4o-mini-2024-07-18": 0.15,
        "o1-preview": 15.00,
        "o1-preview-2024-09-12": 15.00,
        "o1-mini": 3.00,
        "o1-mini-2024-09-12": 3.00,
        "chatgpt-4o-latest": 5.00,
        "gpt-4-turbo": 10.00,
        "gpt-4-turbo-2024-04-09": 10.00,
        "gpt-4": 30.00,
        "gpt-4-32k": 60.00,
        "gpt-4-0125-preview": 10.00,
        "gpt-4-1106-preview": 10.00,
        "gpt-4-vision-preview": 10.00,
        "gpt-3.5-turbo-0125": 0.50,
        "gpt-3.5-turbo-instruct": 1.50,
        "gpt-3.5-turbo-1106": 1.00,
        "gpt-3.5-turbo-0613": 1.50,
        "gpt-3.5-turbo-16k-0613": 3.00,
        "gpt-3.5-turbo-0301": 1.50,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    DeepSeek_input_pricing: Dict[str, float] = {
        "deepseek-chat": 0.14,
        "deepseek-reasoner": 0.55,
    }  # assume always cache miss
    DeepSeek_output_pricing: Dict[str, float] = {
        "deepseek-chat": 0.28,
        "deepseek-reasoner": 2.19,
    }

    # Pricing per 1M output tokens in USD for GPT models
    GPT_output_pricing: Dict[str, float] = {
        "gpt-4o": 10.00,
        "gpt-4o-2024-11-20": 10.00,
        "gpt-4o-2024-08-06": 10.00,
        "gpt-4o-2024-05-13": 15.00,
        "gpt-4o-mini": 0.60,
        "gpt-4o-mini-2024-07-18": 0.60,
        "o1-preview": 60.00,
        "o1-preview-2024-09-12": 60.00,
        "o1-mini": 12.00,
        "o1-mini-2024-09-12": 12.00,
        "chatgpt-4o-latest": 15.00,
        "gpt-4-turbo": 30.00,
        "gpt-4-turbo-2024-04-09": 30.00,
        "gpt-4": 60.00,
        "gpt-4-32k": 120.00,
        "gpt-4-0125-preview": 30.00,
        "gpt-4-1106-preview": 30.00,
        "gpt-4-vision-preview": 30.00,
        "gpt-3.5-turbo-0125": 1.50,
        "gpt-3.5-turbo-instruct": 2.00,
        "gpt-3.5-turbo-1106": 2.00,
        "gpt-3.5-turbo-0613": 2.00,
        "gpt-3.5-turbo-16k-0613": 4.00,
        "gpt-3.5-turbo-0301": 2.00,
        "davinci-002": 2.00,
        "babbage-002": 0.40,
    }

    def __init__(
        self,
        model: str,
        formatted_input_sequence: Optional[List[Dict[str, str]]] = None,
        output_sequence_string: Optional[str] = None,
    ) -> None:
        self.model = model
        self.formatted_input_sequence = formatted_input_sequence
        self.output_sequence_string = output_sequence_string
        self.input_token_length: int = 0
        self.output_token_length: int = 0

    def calculate_input_token_length_GPT(self) -> int:
        """Calculate the number of tokens used by a list of messages."""
        try:
            encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print(
                "Warning: model not found for tokenization. Using cl100k_base encoding for cost "
                "calculation."
            )
            encoding = tiktoken.get_encoding("cl100k_base")

        tokens_per_message = 3
        tokens_per_name = 1

        num_tokens = 0
        if self.formatted_input_sequence:
            for message in self.formatted_input_sequence:
                num_tokens += tokens_per_message
                for key, value in message.items():
                    num_tokens += len(encoding.encode(value))
                    if key == "name":
                        num_tokens += tokens_per_name
        num_tokens += 3  # Every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    def calculate_output_token_length_GPT(self) -> int:
        """
        Calculates the number of tokens in a given output sequence.

        Parameters:
            output_sequence (str): The text output generated by the model.
            model (str): The name of the model for tokenization. Default is "gpt-3.5-turbo".

        Returns:
            int: The number of tokens in the output sequence.
        """
        try:
            # Initialize tokenizer for the specific model
            tokenizer = tiktoken.encoding_for_model(self.model)
        except KeyError:
            print("Warning: Model not found. Using cl100k_base encoding as a fallback.")
            tokenizer = tiktoken.get_encoding("cl100k_base")

        # Tokenize the output sequence
        if self.output_sequence_string:
            tokens = tokenizer.encode(self.output_sequence_string)
            return len(tokens)
        return 0

    def calculate_cost_GPT(self) -> float:
        if self.formatted_input_sequence is not None:
            self.input_token_length = self.calculate_input_token_length_GPT()
            input_cost = self.input_token_length * self.GPT_input_pricing[self.model] / 1e6
        else:
            input_cost = 0
        if self.output_sequence_string is not None:
            self.output_token_length = self.calculate_output_token_length_GPT()
            output_cost = self.output_token_length * self.GPT_output_pricing[self.model] / 1e6
        else:
            output_cost = 0
        return input_cost + output_cost

    def calculate_token_length_DeepSeek(self) -> None:
        module_dir = os.path.dirname(os.path.abspath(__file__))

        tokenizer_relative_path = "tokenizers/deepseek"

        tokenizer_absolute_path = os.path.join(module_dir, tokenizer_relative_path)

        tokenizer = transformers.AutoTokenizer.from_pretrained(
            tokenizer_absolute_path, trust_remote_code=True
        )

        if self.formatted_input_sequence is not None:
            input_sequence = PromptBase.formatted_to_string_OpenAI(self.formatted_input_sequence)
            input_tokenized = tokenizer.encode(input_sequence)
            self.input_token_length = len(input_tokenized)

        if self.output_sequence_string is not None:
            output_tokenized = tokenizer.encode(self.output_sequence_string)
            self.output_token_length = len(output_tokenized)

    def calculate_cost_DeepSeek(self) -> float:
        self.calculate_token_length_DeepSeek()
        cost = (
            self.input_token_length * self.DeepSeek_input_pricing[self.model]
            + self.output_token_length * self.DeepSeek_output_pricing[self.model]
        )
        cost /= 1e6
        return cost
